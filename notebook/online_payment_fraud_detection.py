# -*- coding: utf-8 -*-
"""Online_Payment_Fraud_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WyzdGjj_f_oV_xxVCG9ZF0T_WoB1P6Ax

# Online Payment Fraud Detection üö®üí≥

## Step 1: Problem Definition, Objectives & Data Overview

### 1.1 Project Background  
Financial fraud‚Äîespecially in online payments‚Äîposes a major risk to businesses and consumers alike. Fraudsters exploit gaps in detection systems, leading to large monetary losses and reputational damage.

### 1.2 Primary Objectives  
1. **Detect** potentially fraudulent transactions in real time.  
2. **Maximize** the capture of true frauds (high recall).  
3. **Minimize** false alarms to avoid blocking legitimate customers (high precision).  
4. **Build** an end-to-end ML pipeline that can be iterated and improved over time.

### 1.3 Key Questions to Answer  
- What transaction patterns most strongly indicate fraud?  
- How imbalanced is our dataset, and how should we handle class imbalance?  
- Which features (amount, transaction type, time step, etc.) carry the most predictive power?

### 1.4 Success Metrics  
| Metric     | Why It Matters                                  | Desired Behavior       |
|------------|-------------------------------------------------|------------------------|
| **Recall** | Catch as many frauds as possible                | ‚Üë Higher is better     |
| **Precision** | Limit false positives (legitimate flagged)   | ‚Üë Higher is better     |
| **F1-Score** | Balance between precision and recall          | ‚Üë Higher is better     |
| **ROC-AUC** | Overall separability of fraud vs. non-fraud   | ‚Üë Higher is better     |

### 1.5 Dataset Overview  
We're using the **`jainilcoder/online-payment-fraud-detection`** dataset from Kaggle.  

| Column     | Type      | Description                                            |
|------------|-----------|--------------------------------------------------------|
| `step`     | int       | Discrete time unit (hour) in the simulation            |
| `type`     | object    | Transaction type (`PAYMENT`, `TRANSFER`, etc.)         |
| `amount`   | float     | Amount of transaction                                  |
| `nameOrig` | object    | Customer who initiated the transaction                 |
| `oldbalanceOrg` | float | Balance before the transaction                        |
| `newbalanceOrig` | float | Balance after the transaction                         |
| `nameDest` | object    | Recipient of the transaction                           |
| `oldbalanceDest` | float | Recipient balance before transaction                  |
| `newbalanceDest` | float | Recipient balance after transaction                   |
| `isFraud`  | int (0/1) | Label: **1** = Fraudulent, **0** = Legitimate          |
| `isFlaggedFraud` | int (0/1) | Simulator-generated flag for extremely large transfers |

> **Note:** We'll drop `isFlaggedFraud` in modeling, since it's a simulator artifact.

---
"""

# Step 1.4: Setup & Data Loading

# --- 1. Install / authenticate Kaggle API (if using Colab) ---
# !pip install kaggle
# from google.colab import files
# files.upload()  # upload your kaggle.json here

# --- 2. Import core libraries ---
import os
import pandas as pd          # for data manipulation
import numpy as np           # for numerical operations

# Visualization libraries (will use in EDA)
import matplotlib.pyplot as plt
import seaborn as sns

# Scikit-learn will be used later for modeling
from sklearn.model_selection import train_test_split

# --- 3. Download & load the dataset ---
import os
import kagglehub

path = kagglehub.dataset_download("jainilcoder/online-payment-fraud-detection")

# Construct the full file path to the downloaded CSV file.
file_path = os.path.join(path, "/kaggle/input/online-payment-fraud-detection/onlinefraud.csv")

# Read the csv file into a pandas DataFrame.
df = pd.read_csv(file_path)

# Assuming the CSV is in the working directory:
# data_path = "onlinefraud.csv"
# df = pd.read_csv(data_path)

# Initial Sanity Checks
# -------------------------------------------------
print(f"Loaded dataset with {df.shape[0]} rows and {df.shape[1]} columns.\n")
print("Column data types and non-null counts:")
print(df.info(), "\n")
print(" First 5 rows preview:")
display(df.head())

# Quick Statistics
# -------------------------------------------------
print(" Descriptive statistics for numeric features:")
display(df.describe().T)

print("Class distribution (isFraud):")
display(df['isFraud'].value_counts(normalize=True).rename_axis('isFraud').reset_index(name='proportion'))

"""### Step 1.4: Environment Setup & Data Loading (Detailed)

In this cell, we will:

1. **(Optional)** Install any missing packages (commented out for Colab).  
2. **Import** all required libraries, with a note on their purpose.  
3. **Load** the CSV into a DataFrame, with error handling.  
4. Perform **sanity checks**:
   - Dataset shape  
   - Column data types  
   - Memory usage  
   - Percentage of missing values per column  
   - Count of exact duplicate rows  
5. **Visualize** missing-data patterns with a heatmap.

These steps ensure you fully understand the structure and health of your data before proceeding to EDA.

"""

# ---  Sanity Checks ---
# Shape
n_rows, n_cols = df.shape
print(f"Shape: {n_rows} rows, {n_cols} columns\n")

# Data Types & Non-Null Counts
print("Column info:")
df.info()

# Memory Usage
mem_usage = df.memory_usage(deep=True).sum() / (1024**2)
print(f"\nMemory usage: {mem_usage:.2f} MB\n")

# Missing Values (%)
missing_pct = df.isnull().mean() * 100
print("Missing values (%):")
print(missing_pct.sort_values(ascending=False).round(2), "\n")

# Duplicate Rows
dupes = df.duplicated().sum()
print(f"Duplicate rows: {dupes}\n")

# Preview first 5 rows
print("Data preview:")
display(df.head())

# --- Visualize Missing-Data Pattern ---
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cbar=False, yticklabels=False)
plt.title("Missing Data Heatmap")
plt.show()

# Optional: Barplot of missing value % by column (if any)
missing_pct = df.isnull().mean() * 100
missing_cols = missing_pct[missing_pct > 0]

plt.figure(figsize=(10, 4))
sns.barplot(x=missing_cols.index, y=missing_cols.values, palette='viridis')
plt.title("Missing Value Percentage by Column")
plt.ylabel("Percentage (%)")
plt.xticks(rotation=45)
plt.show()

"""## Step 2: Exploratory Data Analysis & Data Cleaning

### 2.1 Goals of This Step
1. **Understand** each feature's distribution and relationship to fraud.  
2. **Identify** columns that carry little to no predictive power (e.g., IDs).  
3. **Clean** the data by removing or transforming these columns.  
4. **Prepare** the dataset for modeling (train/test split to come later).

### 2.2 Tasks
1. **Value Counts & Unique Values**  
   - Check `type`, `isFlaggedFraud`, etc., to see if a column is constant or near-constant.  
2. **Drop Non-Informative Columns**  
   - Remove transaction IDs (`nameOrig`, `nameDest`) and simulator flags (`isFlaggedFraud`).  
3. **Convert Data Types**  
   - Ensure categorical columns use the `category` dtype for memory & clarity.  
4. **Visualize**  
   - Bar plots for categorical features.  
   - Histograms for numerical features (e.g., `amount`, balances).  
5. **Correlation Analysis**  
   - Heatmap of correlations among numeric features and with the target (`isFraud`).

> **Note:** All cleaning decisions should follow after we've inspected the data distributions.

"""

# --- Value Counts & Uniqueness Checks ---

#  Categorical columns to inspect
cat_cols = ['type', 'isFlaggedFraud']

for col in cat_cols:
    counts = df[col].value_counts(dropna=False)
    pct   = df[col].value_counts(normalize=True, dropna=False) * 100
    print(f"\nColumn `{col}`:\n", pd.concat([counts, pct.rename('percent')], axis=1))

#  Check unique counts for ID‚Äêlike columns
id_cols = ['nameOrig', 'nameDest']
for col in id_cols:
    print(f"\nColumn `{col}` has {df[col].nunique()} unique values "
          f"out of {len(df)} total rows "
          f"({df[col].nunique()/len(df)*100:.2f}% unique).")

# --- Drop Non-Informative Columns ---
# These columns are identifiers or simulator artifacts with no predictive value.
drop_cols = ['nameOrig', 'nameDest', 'isFlaggedFraud']
df_clean = df.drop(columns=drop_cols)
print(f"\n Dropped columns: {drop_cols}")
print("Remaining columns:", df_clean.columns.tolist())

# --- Convert Data Types ---
df_clean['type'] = df_clean['type'].astype('category')

# --- Visualize Categorical Distributions ---
plt.figure(figsize=(6,4))
sns.countplot(x='type', data=df_clean, order=df_clean['type'].cat.categories)
plt.title("Transaction Types")
plt.xlabel("Type")
plt.ylabel("Count")
plt.show()

# Pie chart for transaction type distribution
type_counts = df_clean['type'].value_counts()
plt.figure(figsize=(6,6))
plt.pie(type_counts, labels=type_counts.index, autopct='%1.2f%%', startangle=90, colors=plt.cm.Paired.colors)
plt.title("Distribution of Transaction Types")
plt.show()

# --- Visualize Numerical Distributions ---
num_cols = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig',
            'oldbalanceDest', 'newbalanceDest']
df_clean[num_cols].hist(bins=30, figsize=(12,8), layout=(3,2))
plt.tight_layout()
plt.show()

# --- Correlation Heatmap ---
corr = df_clean[num_cols + ['isFraud']].corr()
plt.figure(figsize=(8,6))
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

# Pie chart for class distribution (isFraud)
fraud_counts = df['isFraud'].value_counts()
plt.figure(figsize=(5,5))
plt.pie(fraud_counts, labels=['Not Fraud', 'Fraud'], autopct='%1.2f%%', startangle=90, colors=['#66b3ff','#ff6666'])
plt.title('Distribution of Fraudulent vs. Non-Fraudulent Transactions')
plt.show()

# Class distribution: Bar plot
plt.figure(figsize=(6,4))
sns.countplot(x='isFraud', data=df_clean)
plt.title("Class Distribution (Fraud vs Non-Fraud)")
plt.xticks([0,1], ['Non-Fraud (0)', 'Fraud (1)'])
plt.ylabel("Count")
plt.show()

# Add percentage display
fraud_rate = df_clean['isFraud'].mean() * 100
print(f"Only {fraud_rate:.4f}% of all transactions are fraudulent.")

# Distribution of amounts
plt.figure(figsize=(10,5))
sns.histplot(data=df_clean[df_clean['amount'] < 100000], x='amount', hue='isFraud', bins=100, kde=False, multiple='stack')
plt.title("Transaction Amount Distribution (under 100k)")
plt.xlabel("Amount")
plt.ylabel("Count")
plt.legend(title='isFraud', labels=['Not Fraud', 'Fraud'])
plt.show()

# Already done earlier, but let's highlight fraud ratio per type again
fraud_by_type = df_clean.groupby('type')['isFraud'].agg(['count', 'sum'])
fraud_by_type['FraudRate (%)'] = (fraud_by_type['sum'] / fraud_by_type['count']) * 100
fraud_by_type = fraud_by_type.sort_values(by='FraudRate (%)', ascending=False)

plt.figure(figsize=(8,4))
sns.barplot(x=fraud_by_type.index, y=fraud_by_type['FraudRate (%)'], palette='magma')
plt.title("Fraud Rate (%) by Transaction Type")
plt.ylabel("Fraud Rate (%)")
plt.xticks(rotation=45)
plt.show()

print(fraud_by_type[['count', 'sum', 'FraudRate (%)']])

plt.figure(figsize=(10,4))
sns.histplot(data=df_clean, x='step', hue='isFraud', bins=100, multiple='stack', palette='coolwarm')
plt.title("Fraud vs Non-Fraud by Time Step (Hourly)")
plt.xlabel("Step (hour)")
plt.ylabel("Transaction Count")
plt.show()

"""##  EDA Summary (Quick Insights)

- **Severe Class Imbalance**  
  Only **0.13%** of transactions are fraud ‚Üí needs **resampling** (SMOTE, undersampling).

- **Fraud Found Only in `TRANSFER` & `CASH_OUT`**  
  These two types account for **all frauds** ‚Üí strong indicator for modeling.

- **Fraud Drains Account**  
  `newbalanceOrig = 0` is common in frauds while `oldbalanceOrg > 0` ‚Üí can create `isDrained` feature.

- **High Transaction Amounts in Fraud**  
  Fraud transactions often involve large amounts ‚Üí apply `log(amount)`.

- **Time Patterns Exist (`step`)**  
  Some hours may show spikes in fraud ‚Üí can extract time-based features later.

## Step 4: Feature Engineering & Data Preparation

### Objective
Use insights from EDA to engineer new features and prepare clean input data for model training.

---

### Key Tasks:
1. **Create custom features**:
   - `isDrained`: Whether origin account was emptied (`old > 0 and new == 0`)
   - (Optional) `isLarge`: Flag unusually large transactions
2. **Log-transform** skewed features (like `amount`)
3. **Encode categorical variables** (`type`)
4. **Drop unhelpful or redundant columns**
5. **Prepare `X` and `y` for training/testing**
"""

# --- Create Engineered Features ---

# One-hot encode 'type' (drop_first avoids dummy trap)
df_fe = pd.get_dummies(df_clean, columns=['type'], drop_first=True)
print("One-hot encoded 'type'. Columns now:\n", df_fe.columns.tolist())

# Feature: Was origin account drained in the transaction?
df_fe['isOrigDrained'] = ((df_fe['oldbalanceOrg'] > 0) & (df_fe['newbalanceOrig'] == 0)).astype(int)

# Feature: Was recipient account untouched? (no change in balance)
df_fe['isDestUntouched'] = ((df_fe['oldbalanceDest'] == df_fe['newbalanceDest'])).astype(int)

# Feature: Was money moved? (old != new)
df_fe['isMoneyMoved'] = ((df_fe['oldbalanceOrg'] != df_fe['newbalanceOrig']) |
                         (df_fe['oldbalanceDest'] != df_fe['newbalanceDest'])).astype(int)

# Feature: Transaction delta (outgoing)
df_fe['deltaOrig'] = df_fe['oldbalanceOrg'] - df_fe['newbalanceOrig']

# Feature: Transaction delta (incoming)
df_fe['deltaDest'] = df_fe['newbalanceDest'] - df_fe['oldbalanceDest']

# Log-transform amount (optional, but helps for modeling)
df_fe['log_amount'] = np.log1p(df_fe['amount'])  # log(1 + x)

# Drop raw 'amount' since we use log, and remove raw balances to prevent leakage
drop_cols = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']
df_model = df_fe.drop(columns=drop_cols)

print("Final columns for modeling:\n", df_model.columns.tolist())

# Train-Test Split with Stratification
from sklearn.model_selection import train_test_split

X = df_model.drop(columns=['isFraud'])
y = df_model['isFraud']

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

print(f" Training set: {X_train.shape[0]} rows")
print(f" Test set: {X_test.shape[0]} rows")
print(f"Fraud ratio in training: {y_train.mean():.5f}")
print(f"Fraud ratio in test    : {y_test.mean():.5f}")

# Handle Class Imbalance (Undersampling)

from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(sampling_strategy=0.3, random_state=42)
X_resampled, y_resampled = rus.fit_resample(X_train, y_train)

print(" After undersampling:")
print(f"Total rows: {len(y_resampled)}")
print(f"Fraud ratio: {y_resampled.mean():.4f}")

"""## Step 5: Baseline Modeling & Evaluation

### Objective
Train a baseline ML model on the engineered + balanced data and evaluate:
- How well it detects fraud
- Where it fails (false positives / false negatives)
- Set performance benchmarks for future models


"""

from sklearn.model_selection import cross_validate, StratifiedKFold
from sklearn.linear_model    import LogisticRegression
from sklearn.tree            import DecisionTreeClassifier
from sklearn.ensemble        import RandomForestClassifier
import xgboost as xgb
import lightgbm as lgb
import pandas as pd
import time

# Define models
models = {
    "LogisticRegression": LogisticRegression(max_iter=200),
    "DecisionTree"       : DecisionTreeClassifier(max_depth=5),
    "RandomForest"       : RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),
    "XGBoost"            : xgb.XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42),
    "LightGBM"           : lgb.LGBMClassifier(n_estimators=100, max_depth=5, random_state=42)
}

# Define cross-validation strategy
cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

# Define scoring metrics
scoring = ['f1', 'accuracy', 'precision', 'recall', 'roc_auc']

# Run cross-validation and store results
results = []
for name, model in models.items():
    start = time.time()
    scores = cross_validate(
        model, X_resampled, y_resampled,
        cv=cv,
        scoring=scoring,
        return_train_score=False,
        n_jobs=-1
    )
    end = time.time()

    results.append({
        "Model": name,
        "F1": scores['test_f1'].mean(),
        "Accuracy": scores['test_accuracy'].mean(),
        "Precision": scores['test_precision'].mean(),
        "Recall": scores['test_recall'].mean(),
        "ROC AUC": scores['test_roc_auc'].mean(),
        "Time (s)": round(end - start, 2)
    })

# Create results dataframe
df_results = pd.DataFrame(results).sort_values(by="F1", ascending=False)

# Print sorted results
print("\n Model Evaluation Results (Sorted by F1 Score):\n")
print(df_results.reset_index(drop=True))

import xgboost as xgb
from sklearn.metrics import classification_report, roc_auc_score

# Final XGBoost model with best default parameters (you can tune this later too)
xgb_model = xgb.XGBClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42,
    n_jobs=-1
)

xgb_model.fit(X_resampled, y_resampled)

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# Feature importances
feat_imp = pd.Series(xgb_model.feature_importances_, index=X_train.columns)
top_feats = feat_imp.sort_values(ascending=False).head(15)

plt.figure(figsize=(8,5))
sns.barplot(x=top_feats.values, y=top_feats.index)
plt.title("Top Feature Importances (XGBoost)")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()

# Predictions on test data
y_pred = xgb_model.predict(X_test)
y_proba = xgb_model.predict_proba(X_test)[:, 1]

# Evaluation
print("Final XGBoost Test Set Evaluation:")
print(classification_report(y_test, y_pred, digits=4))
print("ROC-AUC:", roc_auc_score(y_test, y_proba))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - XGBoost")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Get predicted probabilities (for positive class only)
y_proba = xgb_model.predict_proba(X_test)[:, 1]  # Ensure shape (n_samples,) for binary

# Compute ROC AUC
roc_auc = roc_auc_score(y_test, y_proba)
fpr, tpr, _ = roc_curve(y_test, y_proba)

# Plot ROC curve
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, color='blue', lw=2, label=f"ROC Curve (AUC = {roc_auc:.4f})")
plt.plot([0, 1], [0, 1], 'k--', lw=1.5, label="Random Guess")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC-AUC Curve - XGBoost")
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

"""## Step 6: XGBoost Hyperparameter Tuning

### Objective
Fine-tune the best-performing model (XGBoost) using `RandomizedSearchCV` and evaluate it on the test set.


"""

# 6.1: Parameter Grid for XGBoost
import xgboost as xgb

param_grid = {
    'n_estimators':    [100, 200, 300],
    'max_depth':       [3, 5, 6, 8],
    'learning_rate':   [0.01, 0.05, 0.1, 0.2],
    'subsample':       [0.7, 0.8, 1.0],
    'colsample_bytree':[0.6, 0.8, 1.0],
    'gamma':           [0, 1, 5]
}

# 6.2: Safe Subset Selection for Tuning
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import make_scorer, f1_score

# Determine subset size: up to 100k or full X_resampled if smaller
max_tune = 100_000
n_available = len(X_resampled)
tune_size = min(max_tune, n_available)

# Sample without replacement
X_tune = X_resampled.sample(n=tune_size, random_state=42)
y_tune = y_resampled.loc[X_tune.index]

print(f"Tuning on {tune_size:,} rows (fraud ratio {y_tune.mean():.4f})")

# 6.3: RandomizedSearchCV on the Tuning Subset
xgb_base = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    random_state=42,
    n_jobs=-1
)

xgb_search = RandomizedSearchCV(
    estimator=xgb_base,
    param_distributions=param_grid,
    n_iter=20,
    cv=3,
    scoring=make_scorer(f1_score),
    verbose=2,
    random_state=42,
    n_jobs=-1
)

xgb_search.fit(X_tune, y_tune)
print(" Best XGBoost params:", xgb_search.best_params_)
print(f"Best CV F1-score: {xgb_search.best_score_:.4f}")

# 6.4: Retrain Best XGBoost on Full Resampled Training Set
xgb_best = xgb_search.best_estimator_
xgb_best.fit(X_resampled, y_resampled)

# 6.5: Evaluate Tuned XGBoost on Test Set
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Predictions & probabilities
y_pred  = xgb_best.predict(X_test)
y_proba = xgb_best.predict_proba(X_test)[:, 1]

# Classification report
print("Tuned XGBoost Test Evaluation:")
print(classification_report(y_test, y_pred, digits=4))
print("ROC-AUC:", roc_auc_score(y_test, y_proba))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix ‚Äî Tuned XGBoost")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.05, 0.1]
}

from lightgbm import LGBMClassifier
lgb_model = LGBMClassifier(random_state=42)

search_lgb = GridSearchCV(estimator=lgb_model,
                          param_grid=param_grid,
                          scoring='roc_auc',
                          cv=3,
                          verbose=1,
                          n_jobs=-1)

search_lgb.fit(X_train, y_train)

"""## Step 7: Probability Calibration & Model Ensembling

###  Objectives
1. Calibrate **XGBoost** probabilities for reliable thresholding.  
2. (Optional) Blend with **LightGBM** to combine strengths.  
3. Build a **stacked ensemble** using calibrated predictions and a Logistic Regression meta-learner.  
4. Tune the decision threshold on the stacked outputs to maximize F1.

"""

# 7.1: Probability Calibration
from sklearn.calibration import CalibratedClassifierCV

# Calibrate the tuned XGBoost model
cal_xgb = CalibratedClassifierCV(
    estimator=xgb_best,
    method='isotonic',
    cv=3
)
cal_xgb.fit(X_train, y_train)

# Optional: calibrate LightGBM for blending
from lightgbm import LGBMClassifier
cal_lgb = CalibratedClassifierCV(
    estimator=LGBMClassifier(
        n_estimators=search_lgb.best_params_['lgb__n_estimators'],
        max_depth=search_lgb.best_params_['lgb__max_depth'],
        learning_rate=search_lgb.best_params_['lgb__learning_rate'],
        num_leaves=search_lgb.best_params_['lgb__num_leaves'],
        random_state=42
    ),
    method='isotonic',
    cv=3
)
cal_lgb.fit(X_train, y_train)

# 7.2: Simple Blending
import numpy as np
from sklearn.metrics import classification_report, roc_auc_score

proba_xgb  = cal_xgb.predict_proba(X_test)[:, 1]
proba_lgb  = cal_lgb.predict_proba(X_test)[:, 1]
proba_blend = (proba_xgb + proba_lgb) / 2

# Predictions with 0.5 threshold
y_pred_blend = (proba_blend >= 0.5).astype(int)
print("Blended Model Report (@0.5 threshold):")
print(classification_report(y_test, y_pred_blend, digits=4))
print("ROC-AUC:", roc_auc_score(y_test, proba_blend))

# 7.3: Stacking Ensemble
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression

# Prepare meta-feature DataFrames
train_meta = pd.DataFrame({
    'xgb': cal_xgb.predict_proba(X_train)[:, 1],
    'lgb': cal_lgb.predict_proba(X_train)[:, 1]
}, index=X_train.index)

test_meta = pd.DataFrame({
    'xgb': cal_xgb.predict_proba(X_test)[:, 1],
    'lgb': cal_lgb.predict_proba(X_test)[:, 1]
}, index=X_test.index)

# Train meta-learner
meta_clf = LogisticRegression(max_iter=500, random_state=42)
meta_clf.fit(train_meta, y_train)

# Stacked predictions
proba_stack = meta_clf.predict_proba(test_meta)[:, 1]
y_pred_stack = (proba_stack >= 0.5).astype(int)

print("üîé Stacked Ensemble Report (@0.5 threshold):")
print(classification_report(y_test, y_pred_stack, digits=4))
print("ROC-AUC (stacked):", roc_auc_score(y_test, proba_stack))

# 7.4: Threshold Tuning for Stacked Ensemble
from sklearn.metrics import f1_score

best_t, best_f1 = 0, 0
for t in np.linspace(0.01, 0.99, 99):
    f1 = f1_score(y_test, (proba_stack >= t).astype(int))
    if f1 > best_f1:
        best_f1, best_t = f1, t

print(f"Optimal threshold = {best_t:.2f} ‚Üí F1-Score = {best_f1:.4f}")

# Evaluate at optimal threshold
y_pred_opt = (proba_stack >= best_t).astype(int)
print("Stacked Ensemble Report (@optimal threshold):")
print(classification_report(y_test, y_pred_opt, digits=4))
print("ROC-AUC (optimal):", roc_auc_score(y_test, proba_stack))

"""## üìù Summary of Step 7

- **Calibrated XGBoost** (and LightGBM) probabilities for reliable thresholds.  
- **Blended** by averaging XGB+LGB predictions.  
- Built a **stacked ensemble** with a Logistic Regression meta-learner.  
- **Tuned** the decision threshold to maximize F1.  
- Evaluated all approaches on the test set, comparing precision, recall, F1, and ROC-AUC.
"""

# --- Save the trained model and feature columns ---
import joblib
import os

# Ensure model directory exists
os.makedirs("../model", exist_ok=True)

# Save the tuned XGBoost model
joblib.dump(xgb_best, "../model/xgb_fraud_model.pkl")

# Save the feature columns used for prediction
feature_columns = list(X_train.columns)
joblib.dump(feature_columns, "../model/feature_columns.pkl")

print("Model and feature columns saved to ../model/")